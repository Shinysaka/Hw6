# Hw6

The provided code defines a simple neural network for binary classification with two hidden layers (20 and 4 neurons) using the ReLU activation function. The model is compiled with the Adam optimizer and binary crossentropy loss. Training occurs for 100 epochs, and the model is evaluated on test data. The summary prints the architecture details, and the final evaluation results (loss and accuracy) are displayed.
